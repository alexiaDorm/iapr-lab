{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 â€’  Classification\n",
    "\n",
    "\n",
    "**Group ID:** 32\n",
    "\n",
    "**Author 1 (sciper):** Alexia Dormann (301997)\n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 19.04.2023  \n",
    "**Due date:** 05.05.2023 \n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as\n",
    "preparation for the final project, which is a practical project which ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "We will not rerun the notebook for you.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use PyTorch. If you are not familiar with this library, [here](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) is a quick tutorial of the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(platform.system())\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install torch==1.8.1 torchvision==0.9.1\n",
    "else:\n",
    "    %pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "#with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    #tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 - Out-of-Distribution detection in colorectal cancer histology (12 points)\n",
    "\n",
    "Colorectal cancer is one of the most widespread cancers for men and women. Diagnosis complemented with prognostic and predictive biomarker information is essential for patient monitoring and applying personalized treatments. A critical marker is the tumor/stroma ratio in unhealthy tissues sampled from the colon. The higher the ratio, the more invasive the cancer is. The degree of invasion is tightly linked to patient survial probability.\n",
    "\n",
    "To measure the ratio, a pathologist needs to analyze the unhealthy tissue under a microscope and estimate it from a look. As the number of samples to analyze is huge and estimations are only sometimes precise, automatic recognition of the different tissue types in histological images has become essential. Such an automatic process requires the development of a multi-class classifier to identify the numerous tissues. As shown below, they are usually 8 tissue types to categorize: TUMOR, STROMA, LYMPHO (lymphocytes), MUCOSA, COMPLEX (complex stroma), DEBRIS, ADIPOSE and EMPTY (background).\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"../data/lab-03-data/part1/kather16.svg\" width=\"1100\">\n",
    "    <center>\n",
    "    <figcaption>Fig1: Collection of tissue types in colorectal cancer histology (Kather-16)</figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "Up to this day, state-of-the-art methods use deep-learning-based supervised learning methods. A downfall of such an approach is the necessity to access a well-annotated training dataset. In histology, annotating data is difficult. It is time-consuming and requires the expertise of pathologists. Moreover, the annotator must label every tissue type while only two (TUMOR and STROMA) are interesting. \n",
    "\n",
    "\n",
    "Consequently, we propose another approach. In order to make the annotation task less tedious, we ask the annotator to label only the tissues of interest and dump the others. Then, we must train a binary classifier to automatically recognize these tissues at test time. In this part, you will implement the proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Binary classifier with Mahalanobis distance (3 points)\n",
    "\n",
    "Based on the abovementioned process, your task is to build a model that recognizes TUMOR (Label 0) and STROMA (Label 1) tissue types. Your model will be supervised by a training dataset containing TUMOR and STROMA annotations; note that all other tissues have been dropped.\n",
    "We will not ask you to train a deep-learning-based binary classifier from scratch. Instead, we provide excellent features (descriptors) of the images we extracted from a visual foundation model. (Note: As the nature of the foundation model is not part of this lecture, feel free to ask TAs if you are curious).\n",
    "\n",
    "Run the cell below to extract the provided train and test dataset. Each image is represented by a 768-d feature vector extracted from a visual foundation model. The train and test datasets contain feature vectors of 878 and 186 images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([186, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Label mapping\n",
    "label_to_classname = {0 : \"TUMOR\", 1 : \"STROMA\"}\n",
    "\n",
    "# Train features and labels\n",
    "train_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_features.pth\"))\n",
    "train_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_labels.pth\"))\n",
    "\n",
    "# Test features and labels\n",
    "test_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_features.pth\"))\n",
    "test_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_labels.pth\"))\n",
    "\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (2.5 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using minimum Mahalanobis distance.\n",
    "\n",
    "*Note:* You are not allowed to use any prebuilt Mahalanobis distance function. Additionally, ```torch.cov``` is not defined to compute the covariance matrix. You can use ```sklearn.covariance.LedoitWolf``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from numpy.linalg import inv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_mahalanobis_distance(x, mean_train, cov_train):\n",
    "    #Compute mahalanobis distance to the set of datapoints\n",
    "    \n",
    "    distance = torch.matmul((x - mean_train).view(1,-1), torch.from_numpy(cov_train))  \n",
    "    distance = torch.matmul(distance, (x - mean_train).view(-1,1)).item()\n",
    "    distance = np.sqrt(distance)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def classifier_min_mahalanobis(x, mean_train, cov_train):\n",
    "    #Compute mahalanobis distance to each group type\n",
    "    distances = {}\n",
    "    for i, mean in enumerate(mean_train):\n",
    "        distances[i] = compute_mahalanobis_distance(x, mean, cov_train[i])\n",
    "    \n",
    "    distances = pd.Series(distances)\n",
    "    \n",
    "    #Find min dist and group closest to test point\n",
    "    min_dist = distances.min()\n",
    "    label_x = distances.idxmin()\n",
    "    \n",
    "    return min_dist, label_x\n",
    "\n",
    "def compute_mean_cov_train(train, train_labels, tissue_type):\n",
    "    mean, cov = [], []\n",
    "    for tissue in tissue_type:\n",
    "        idx_tissue = np.where(train_labels == tissue)\n",
    "        mean.append(torch.mean(train[idx_tissue], dim=0))\n",
    "        cov.append(inv(LedoitWolf().fit(train[idx_tissue]).covariance_))\n",
    "        \n",
    "    return mean, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute mean and cov for each tissue type\n",
    "mean_train, cov_train = compute_mean_cov_train(train_features, train_labels, label_to_classname.keys())\n",
    "\n",
    "#Predict for each test datapoint\n",
    "pred = [classifier_min_mahalanobis(x, mean_train, cov_train) for x in test_features]\n",
    "\n",
    "min_dist =[x[0] for x in pred]\n",
    "pred = [x[1] for x in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 points)** Compute the accuracy of your predictions with the test labels (```test_labels```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the prediction using the Mahalanobis distance for each tissue is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.95698925])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Compute accuracy for each class\n",
    "matrix = confusion_matrix(test_labels, pred)\n",
    "print('The accuracy of the prediction using the Mahalanobis distance for each tissue is')\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Out-of-Distribution detection with Mahalanobis distance (3 points)\n",
    "\n",
    "You will note that the test you run above is not really realistic. Like the training set, it contains only the TUMOR and STROMA tissue types. Nevertheless, at test time, the other tissues (Label -1) are also present and cannot be filtered by hand. Moreover, they cannot be recognized by the model as they are out of the training distribution (It is the consequence of the laziness of the annotators ;)). For this reason, it is essential to filter them out. This task is called Out-of-Distribution (OoD) detection. \n",
    "\n",
    "A simple way to do OoD detection is to compute for every test example an OoD-ness score which should be low for In-Distribution (ID) examples and high for OoDs. Then we define a threshold from which every example with an OoD-ness lying above is discarded, and those lying below are forwarded to the model for prediction. An example of OoD-ness score is the minimum Mahalanobis distance.\n",
    "\n",
    "Run the cell below to load a new test set containing OoD examples. It has 186 ID and 558 OoD examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([744, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_classname_w_ood = {0 : \"TUMOR\", 1 : \"STROMA\", -1 : \"OoD\"}\n",
    "\n",
    "# Test features and labels with OoD tissues\n",
    "test_features_w_ood = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test2_features.pth\"))\n",
    "test_labels_w_ood = torch.load(os.path.join(data_base_path, data_folder,\"part1/k16_test2_labels.pth\"))\n",
    "\n",
    "test_features_w_ood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (0.5 point)** Why do you think the minimum Mahalanobis distance is a good OoD-ness score?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "If we assume that the Mahalanobis distance is a good estimate of how similar two points are, then, if a point has a very large distance from all the datapoints in the training set it means that it is not very similar to any of the point in the training datasets. Therefore, we can conclude that it is most likely don't belong to any of the tissue type in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 point)** Compute the minimum Mahalanobis distance for every test examples in ```test_features_w_ood``` with respect to the training features (```train_features```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73.65674718754555,\n",
       " 82.46884473279894,\n",
       " 79.58225893690125,\n",
       " 82.35573323621738,\n",
       " 82.14357785495163,\n",
       " 78.91178198429877,\n",
       " 83.1703353816732,\n",
       " 81.55448655557676,\n",
       " 76.99338255351073,\n",
       " 78.06131779529154]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 2\n",
    "#Compute mahalanobis distance to each tissue type\n",
    "min_distances = [classifier_min_mahalanobis(test, mean_train, cov_train)[0] for test in test_features_w_ood]\n",
    "min_distances[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (0.5 point)** Plot a histogram to show the difference between the Mahalanobis distance of TUMOR, STROMA and OoD tissue types and comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Task 3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Compute distances for each tissue type\n",
    "distances_tissues = {}\n",
    "for tissue in label_to_classname_w_ood.keys():\n",
    "    distances_tissues[label_to_classname_w_ood[tissue]] = np.array(min_distances)[test_labels_w_ood == tissue]\n",
    "\n",
    "#Plot the histograms\n",
    "fig, axes = plt.subplots(1,3, figsize=(20, 8))\n",
    "for i, tissue in enumerate(label_to_classname_w_ood.values()):\n",
    "    axes[i].hist(distances_tissues[tissue])\n",
    "    axes[i].set_title('Histogram of min Mahalanobis of ' + tissue + ' tissue')\n",
    "    axes[i].set_xlim(0,90)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** We see that for the OoD tissue they are two type of images, one with high distances and one with simlar values as for the tumor and stroma tissues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Find a threshold on the Mahalanobis distance such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of examples filtered out for:\n",
      "TUMOR  :  58.064516129032256\n",
      "STROMA  :  73.11827956989248\n",
      "OoD  :  94.26523297491039\n"
     ]
    }
   ],
   "source": [
    "### Task 4\n",
    "#Manually define threshold from histograms above\n",
    "thres = 31\n",
    "\n",
    "#Filter out OoD examples\n",
    "#Get idx of sample with min dist lower than threshold -> keep\n",
    "idx_OoD = np.argwhere([dist < thres for dist in min_distances])\n",
    "test_filtered = np.squeeze(np.array(test_features_w_ood[idx_OoD,:])); test_filtered_labels = np.squeeze(np.array(test_labels_w_ood[idx_OoD]))\n",
    "\n",
    "#Compute percentage of filtered samples for each tissue type\n",
    "print('Percentage of examples filtered out for:')\n",
    "for tissue_type in label_to_classname_w_ood.keys():\n",
    "    perc = (test_filtered_labels[np.where(test_filtered_labels == tissue_type)].shape[0]/test_labels_w_ood[np.where(test_labels_w_ood == tissue_type)].shape[0])* 100\n",
    "    print(label_to_classname_w_ood[tissue_type], ' : ', 100 - perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (0.5 point)** Assign prediction -1 to filtered out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```). Is it satisfactory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the prediction using the Mahalanobis distance for each tissue is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94265233, 0.41935484, 0.2688172 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 5\n",
    "#Get filtered idx\n",
    "idx_OoD = np.argwhere([dist > thres for dist in min_distances])\n",
    "\n",
    "#Predict using min distance\n",
    "pred = np.array([classifier_min_mahalanobis(x, mean_train, cov_train)[1] for x in test_features_w_ood])\n",
    "\n",
    "#Assign -1 pred to filtered out example\n",
    "pred[idx_OoD] = -1\n",
    "\n",
    "#Compute accuracy for each class\n",
    "matrix = confusion_matrix(test_labels_w_ood, pred)\n",
    "print('The accuracy of the prediction using the Mahalanobis distance for each tissue is')\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** The accuracy is not very good. Lot of the TUMOR + STROMA img are filtered out using this method of outliers detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Out-of-distribution detection with k-NN classifier (6 points)\n",
    "\n",
    "The visual foundation models are known to be very good k-NN classifiers. It motivates us to implement a k-NN classifier to recognize TUMOR and STROMA. Moreover, k-NN distance is a good OoD-ness score and suits our task.\n",
    "\n",
    "**Task 1 (2 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using a k-NN classifier. Then report the accuracy of your predictions with the test labels (```test_labels```).\n",
    "\n",
    "*Note:* The choice of `k` is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Task 1\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_neigh = np.arange(1,16)\n",
    "score = []\n",
    "for k in n_neigh:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(train_features, train_labels)\n",
    "    \n",
    "    pred = neigh.predict(test_features)\n",
    "    score.append(accuracy_score(test_labels, pred))\n",
    "    \n",
    "plt.plot(score)\n",
    "plt.title('Accuracy of k-NN classifier in function of k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 points)** Perform OoD detection on the test features (```test_features_w_ood```) using a k-NN distance based OoD-ness score. Find a threshold on your OoD-ness score such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out? Finally, assign prediction -1 to filter out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```).\n",
    "\n",
    "*Note:* The OoD-ness is based on the distance to the k-nearest neighbors. The formulation is up to you. You have to justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (1 point)** Is k-NN better than Mahalanobis distance ? Make an hypothesis for the reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Do you think we can suggest the approach presented in this exercise to compute TUMOR/STROMA ratio automatically ? Justify your thoughs. If not, suggest at least two ideas to improve it.\n",
    "\n",
    "*Note:* Annotating all the training dataset is not an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 (12 points)\n",
    "In this part, we aim to classify cervical cells resulting from Pap smear tests. To that end we'll be using a publicly available cell dataset: Sipakmed (https://www.cs.uoi.gr/~marina/sipakmed.html). The dataset is composed of 4049 images of isolated cells cropped from 966 cluster cell images of Pap smear slides. Each cell in the dataset has been categorized in either of the following categories: \n",
    "\n",
    "    - Superficial-Intermediate.\n",
    "    - Parabasal.\n",
    "    - Koilocytotic.\n",
    "    - Dysketarotic.\n",
    "    - Metaplastic.\n",
    "Your objective is to implement a classifier to automate the cell classification process. To ease your work we provide you with pre-computed embeddings for each images (`lab-03-data/part2/sipakmed_clean_embeddings.pth`). The embeddings are obtained from a pre-trained ResNet-50 (https://arxiv.org/pdf/1512.03385.pdf) and the corresponding images are also provided (`lab-03-data/part2/sipakmed_clean`). Note that you are free to discard the provided embeddings and work directy with the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset (4 points)\n",
    "Your first task is prepare the dataset such that it can be used to train your model. For that purpose we prepared the skeleton of the class `Sipakmed` that inherits from the class `Dataset` of PyTorch. Read the documentation (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) and complete the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import os\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features\n",
    "features_path = '../data/lab-03-data/part2/sipakmed_clean_embeddings.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sipakmed(Dataset):\n",
    "    phase_dict = {\n",
    "            'train': {'start': 0.0, 'stop': 0.5},\n",
    "            'val': {'start': 0.5, 'stop': 0.75},\n",
    "            'test': {'start': 0.75, 'stop': 1.0}\n",
    "    }\n",
    "    label_dict = {\n",
    "        'im_Superficial-Intermediate': 0,\n",
    "        'im_Parabasal': 1, \n",
    "        'im_Metaplastic': 2,\n",
    "        'im_Koilocytotic': 3,\n",
    "        'im_Dyskeratotic': 4\n",
    "    }\n",
    "    \n",
    "    def __init__(self, features_path, phase):\n",
    "\n",
    "        super(Sipakmed, self).__init__()\n",
    "        # Store class attributes\n",
    "        self.phase = phase\n",
    "        \n",
    "        # Collect the data \n",
    "        import torch\n",
    "        import torch.nn.functional as F\n",
    "        import numpy as np\n",
    "        self.raw_data = torch.load(features_path)\n",
    "        self.features, self.labels, self.paths = self.collect_data()\n",
    "        \n",
    "    def collect_data(self):\n",
    "        # Iterate over the dirs/classes\n",
    "        features, labels, paths = [], [], []\n",
    "        for dir_name, dir_dict in self.raw_data.items():\n",
    "            # Get the paths and embeddings\n",
    "            dir_paths, dir_embeddings = list(zip(*[(k, v) for k, v in dir_dict.items()]))\n",
    "            \n",
    "            # Split\n",
    "            n = len(dir_paths)\n",
    "            np.random.seed(42)\n",
    "            permutations = np.random.permutation(n)\n",
    "            dir_paths = np.array(dir_paths)[permutations]\n",
    "            dir_embeddings = torch.stack(dir_embeddings)[permutations]\n",
    "            n_start = int(n * self.phase_dict[self.phase]['start'])\n",
    "            n_stop = int(n * self.phase_dict[self.phase]['stop'])\n",
    "            dir_embeddings = dir_embeddings[n_start: n_stop]\n",
    "            dir_paths = dir_paths[n_start: n_stop]\n",
    "    \n",
    "            # Store\n",
    "            features.append(dir_embeddings)\n",
    "            paths.append(dir_paths)\n",
    "            dir_labels = torch.tensor([self.label_dict[p.split('/')[-2]] for p in dir_paths])\n",
    "            labels.append(dir_labels)\n",
    "            \n",
    "        # Merge\n",
    "        features = torch.cat(features)\n",
    "        labels = torch.cat(labels)\n",
    "        paths = np.concatenate(paths)\n",
    "        return features, labels, paths\n",
    "            \n",
    "        \n",
    "    def __len__(self,):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.labels.size()[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the embedding, label, and image path of queried index.\n",
    "        \"\"\"\n",
    "        embedding = self.features[index,:]\n",
    "        label = self.labels[index]\n",
    "        path = self.paths[index]\n",
    "        \n",
    "        return embedding, label, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the implementation of `Sipakmed` completed, create 3 instances of the class (train/val/test) with the corresponding `phase` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the datasets\n",
    "train_dataset = Sipakmed(features_path, 'train')\n",
    "val_dataset = Sipakmed(features_path, 'val')\n",
    "test_dataset = Sipakmed(features_path, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your datasets are ready, use the class `DataLoader` from PyTorch to let it handle efficiently the batching, shuffling, etc. of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to know your data. Plot a few example images for each class of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "\n",
    "# Visualize some training example\n",
    "img_name = ['001_01', '002_05', '021_02', '069_06']\n",
    "path_img = ['../data/lab-03-data/part2/sipakmed_clean/' + y + '/' + x + '.jpg'  for x in img_name for y in train_dataset.raw_data.keys()]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "\n",
    "for ax, path in zip(axes, path_img):\n",
    "    img = skimage.io.imread(path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training (4 points)\n",
    "In this part your objective is to implement the required tools to train your model. The first thing you'll need is a a model which takes as input the pre-computed features and returns the corresponding class probabilities/logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the model\n",
    "embedding_dim = train_dataset.features.shape[1]\n",
    "hidden_dim = 150\n",
    "\n",
    "class CellsImgClassifier(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "        self.activation_func = torch.tanh\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "     \n",
    "    def forward(self, input):\n",
    "        layer1 = self.activation_func(self.linear1(input) )\n",
    "        logits = self.log_softmax(self.linear2(layer1))\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "model = CellsImgClassifier(embedding_dim, hidden_dim, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer will keep track of your model's parameters, gradients, etc (https://pytorch.org/docs/stable/optim.html). It is responsible to update your model's parameters after each forward pass using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=0.001,\n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that takes as input the model's output and the corresponding labels and returns the perÃ§entage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of predictions based on the model outputs (NxK: N samples, K classes) \n",
    "    and the labels (N: N samples).\n",
    "    \"\"\"\n",
    "    pred = torch.argmax(outputs, dim=1)\n",
    "    acc = confusion_matrix(labels, pred)\n",
    "    acc = acc.diagonal()/acc.sum(axis=1)\n",
    "    \n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `train` that forwards the complete training set through your model (= 1 epoch) and updates its parameters after each forward pass. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, loader):\n",
    "    # Set the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths \n",
    "        input, label, path = batch\n",
    "        \n",
    "        # Feed the embeddings to the model\n",
    "        out = model(input)\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store the outputs, labels and loss\n",
    "        full_outputs.append(out); full_labels.append(label); losses.append(loss)\n",
    "    \n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    acc = accuracy(full_outputs, full_labels)\n",
    "    \n",
    "    return acc, full_outputs, full_labels, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `validate` that forwards the complete validation or test set through your model and evaluates its predictions. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, criterion, loader):\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    full_paths = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths\n",
    "        input, label, path = batch\n",
    "        \n",
    "        # Feed the embeddings to the model\n",
    "        out = model(input)\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        # Store the outputs, labels and loss\n",
    "        full_outputs.append(out); full_labels.append(label); losses.append(loss)\n",
    "        full_paths.append(path)\n",
    "    \n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "    full_paths = np.concatenate(full_paths)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    acc = accuracy(full_outputs, full_labels)\n",
    "    \n",
    "    return acc, full_outputs, full_labels, losses, full_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to train you model. Alternate between training and validation steps to find and save the best model (best accuracy on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "epochs = 100\n",
    "best_acc = 0\n",
    "model_savepath = '../data'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    train_acc, _, _, _ = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    # Evaluate\n",
    "    val_acc, _, _, _, _ = validate(model, criterion, val_loader)\n",
    "    \n",
    "    # Save the model\n",
    "    if val_acc > best_acc:\n",
    "        torch.save(model.state_dict(), model_savepath+'/CellClassifierNN.pt')\n",
    "        best_acc = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation (4 points)\n",
    "Re-load the best model and evaluate its predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-load the best model\n",
    "model = CellsImgClassifier(embedding_dim, hidden_dim, 5)\n",
    "model.load_state_dict(torch.load(model_savepath+'/CellClassifierNN.pt'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, pred, labels, _, _ = validate(model, criterion, test_loader)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful tool to analyze your model's performance on the different classes is the confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). Computes its entries for your model and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[415,   0,   0,   0,   0],\n",
       "       [  0, 393,   0,   0,   0],\n",
       "       [  0,   0, 396,   0,   0],\n",
       "       [  0,   0,   0, 412,   0],\n",
       "       [  0,   0,   0,   0, 406]], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "pred = torch.argmax(pred, dim=1)\n",
    "conf_matrix = confusion_matrix(labels, pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it can be useful to plot the problematic samples as well as the predicted and ground truth classes. Can you do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified samples\n",
    "### YOUR CODE\n",
    "\n",
    "# Plot the misclassified samples\n",
    "### YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
